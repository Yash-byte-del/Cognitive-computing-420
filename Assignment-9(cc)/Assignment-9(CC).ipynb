{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2c56f4-807a-4529-bd72-c5d8dcc113d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5b818a-9d68-4f10-b2f7-380c1e5c73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90c1485a-6533-48e2-9240-24a9556d114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency Distribution (excluding stopwords):\n",
      "[('topper', 1), ('chapra', 1), ('university', 1)]\n",
      "\n",
      "Original text: I am a topper from chapra university.\n",
      "\n",
      "Lowercase without punctuation: i am a topper from chapra university\n",
      "\n",
      "Word tokens: ['i', 'am', 'a', 'topper', 'from', 'chapra', 'university']\n",
      "\n",
      "Sentence tokens: ['I am a topper from chapra university.']\n",
      "\n",
      "Filtered words (no stopwords): ['topper', 'chapra', 'university']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data (including punkt_tab)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  # Required for sentence tokenization\n",
    "\n",
    "# Sample paragraph about space exploration\n",
    "paragraph = \"\"\"I am a topper from chapra university.\"\"\"\n",
    "\n",
    "# 1. Convert to lowercase and remove punctuation\n",
    "lower_text = paragraph.lower()\n",
    "no_punct = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 2. Tokenize into words and sentences\n",
    "words = word_tokenize(no_punct)\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# 3. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 4. Display word frequency distribution\n",
    "fdist = FreqDist(filtered_words)\n",
    "print(\"Word Frequency Distribution (excluding stopwords):\")\n",
    "print(fdist.most_common())\n",
    "\n",
    "# Print intermediate results for verification\n",
    "print(\"\\nOriginal text:\", paragraph)\n",
    "print(\"\\nLowercase without punctuation:\", no_punct)\n",
    "print(\"\\nWord tokens:\", words)\n",
    "print(\"\\nSentence tokens:\", sentences)\n",
    "print(\"\\nFiltered words (no stopwords):\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4b54b7b-3c8a-474a-8838-b9b8f49d8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['topper', 'chapra', 'university']\n",
      "\n",
      "--- Stemming Results ---\n",
      "Porter Stemmer: ['topper', 'chapra', 'univers']\n",
      "Lancaster Stemmer: ['top', 'chapr', 'univers']\n",
      "\n",
      "--- Lemmatization Results ---\n",
      "WordNet Lemmatizer: ['topper', 'chapra', 'university']\n",
      "\n",
      "--- Comparison ---\n",
      "Word\t\tPorter\t\tLancaster\tLemma\n",
      "--------------------------------------------------\n",
      "topper\t\ttopper\t\ttop\t\ttopper\n",
      "chapra\t\tchapra\t\tchapr\t\tchapra\n",
      "university\t\tunivers\t\tunivers\t\tuniversity\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Sample filtered words from Q1 (after stopword removal)\n",
    "filtered_words = ['topper', 'chapra', 'university']\n",
    "\n",
    "# Initialize stemmers and lemmatizer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "porter_stems = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stems = [lancaster.stem(word) for word in filtered_words]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "# Display results\n",
    "print(\"Original Words:\", filtered_words)\n",
    "print(\"\\n--- Stemming Results ---\")\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "print(\"\\n--- Lemmatization Results ---\")\n",
    "print(\"WordNet Lemmatizer:\", lemmas)\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Word\\t\\tPorter\\t\\tLancaster\\tLemma\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(filtered_words)):\n",
    "    print(f\"{filtered_words[i]}\\t\\t{porter_stems[i]}\\t\\t{lancaster_stems[i]}\\t\\t{lemmas[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7e5f9b-98fa-4ed9-9ea7-f65c9fbb3d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regular Expression Results ===\n",
      "\n",
      "a) Words with >5 letters: ['topper', 'chapra', 'university']\n",
      "\n",
      "b) Numbers in text: ['12345']\n",
      "\n",
      "c) Capitalized words: ['I']\n",
      "\n",
      "=== Text Splitting Results ===\n",
      "\n",
      "a) Alphabetic words only: ['I', 'am', 'a', 'topper', 'from', 'chapra', 'university']\n",
      "\n",
      "b) Words starting with vowels: ['I', 'am', 'a', 'university']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original text from Q1\n",
    "text = \"I am a topper from chapra university.\"\n",
    "\n",
    "### Regular Expression Tasks ###\n",
    "print(\"=== Regular Expression Results ===\")\n",
    "\n",
    "# a. Extract words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
    "print(\"\\na) Words with >5 letters:\", long_words)\n",
    "\n",
    "# b. Extract numbers (if any) - Example added for demonstration\n",
    "text_with_num = text + \" My roll number is 12345.\"\n",
    "numbers = re.findall(r'\\b\\d+\\b', text_with_num)\n",
    "print(\"\\nb) Numbers in text:\", numbers)\n",
    "\n",
    "# c. Extract capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "print(\"\\nc) Capitalized words:\", capitalized_words)\n",
    "\n",
    "### Text Splitting Tasks ###\n",
    "print(\"\\n=== Text Splitting Results ===\")\n",
    "\n",
    "# a. Split into alphabetic words (no digits/special chars)\n",
    "alpha_only = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "print(\"\\na) Alphabetic words only:\", alpha_only)\n",
    "\n",
    "# b. Extract words starting with vowels\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b', text)\n",
    "print(\"\\nb) Words starting with vowels:\", vowel_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2a2cc7-1db4-4019-a25c-79adf4000661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Custom Tokenization ===\n",
      "[\"I'm\", 'a', 'topper', 'from', 'chapra', 'university', 'Contact', 'me', 'at', 'test', 'example', 'com', 'visit', 'https', 'chapra', 'edu', 'or', 'call', '91', '9876543210', '123', '456', '7890', 'My', 'GPA', 'is', '3', '14', 'and', 'I', 'love', 'state', 'of', 'the', 'art', 'tech']\n",
      "\n",
      "=== Cleaned Text ===\n",
      "I'm a topper from chapra university. Contact me at <EMAIL>, \n",
      "visit <URL> or call <PHONE> / <PHONE>. \n",
      "My GPA is 3.14 and I love state-of-the-art tech!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original text from Q1 with added complex elements for demonstration\n",
    "text = \"\"\"I'm a topper from chapra university. Contact me at test@example.com, \n",
    "visit https://chapra.edu or call +91 9876543210 / 123-456-7890. \n",
    "My GPA is 3.14 and I love state-of-the-art tech!\"\"\"\n",
    "\n",
    "### Custom Tokenization Function ###\n",
    "def custom_tokenize(text):\n",
    "    # Pattern explanation:\n",
    "    # - \\b[\\w']+\\b : Matches words including contractions (e.g., \"I'm\")\n",
    "    # - \\b\\d+\\.\\d+\\b : Matches decimal numbers (e.g., 3.14)\n",
    "    # - \\b\\d+\\b : Matches standalone numbers\n",
    "    # - \\b\\w+(?:-\\w+)+\\b : Matches hyphenated words (e.g., \"state-of-the-art\")\n",
    "    pattern = r\"\"\"\n",
    "        \\b[\\w']+\\b|          # Words with contractions\n",
    "        \\b\\d+\\.\\d+\\b|        # Decimal numbers\n",
    "        \\b\\d+\\b|             # Standalone numbers\n",
    "        \\b\\w+(?:-\\w+)+\\b     # Hyphenated words\n",
    "    \"\"\"\n",
    "    return re.findall(pattern, text, re.VERBOSE)\n",
    "\n",
    "tokens = custom_tokenize(text)\n",
    "print(\"=== Custom Tokenization ===\")\n",
    "print(tokens)\n",
    "\n",
    "### Regex Substitutions ###\n",
    "def clean_text(text):\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    # Replace phone numbers (supports +91 1234567890 and 123-456-7890 formats)\n",
    "    text = re.sub(r'\\+\\d{1,3}\\s\\d{5,15}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', text)\n",
    "    return text\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "print(\"\\n=== Cleaned Text ===\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0650d-0160-48a6-b45f-90e3dc37d674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
